{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872cb05c",
   "metadata": {},
   "source": [
    "# Approximation paramétrique de $Q(s,a)$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94189ffb",
   "metadata": {},
   "source": [
    "## 1. Approximation de la fonction valeur d’action\n",
    "\n",
    "Lorsque l’espace d’états $S$ est continu ou très grand, on approxime la fonction $Q(s,a)$ par un modèle paramétrique :\n",
    "\n",
    "$$\n",
    "Q_\\theta(s,a)\n",
    "$$\n",
    "\n",
    "où $\\theta \\in \\mathbb{R}^d$ désigne les paramètres.\n",
    "\n",
    "On suppose que l’espace d’actions $A$ est discret alors que nous n'impososn pas ça sur S. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec29e4",
   "metadata": {},
   "source": [
    "## 2. Problème d’approximation\n",
    "\n",
    "On cherche à approximer la fonction $Q_\\pi(s,a)$ associée à une politique $\\pi$.\n",
    "\n",
    "On définit la fonction coût quadratique :\n",
    "\n",
    "$$\n",
    "J(\\theta)\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "\\left(\n",
    "Q_\\pi(s,a) - Q_\\theta(s,a)\n",
    "\\right)^2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "où l’espérance est prise selon la distribution induite par la politique $\\pi$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5d334",
   "metadata": {},
   "source": [
    "## 3. Gradient exact\n",
    "\n",
    "Le gradient s’écrit :\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "\\left(\n",
    "Q_\\theta(s,a) - Q_\\pi(s,a)\n",
    "\\right)\n",
    "\\nabla_\\theta Q_\\theta(s,a)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c08b0d",
   "metadata": {},
   "source": [
    "## 4. Descente de gradient stochastique\n",
    "\n",
    "On cherche à estimer le meilleur $\\theta$, pour cela on fait une descente de gradient\n",
    "\n",
    "$$\n",
    "\\theta\n",
    "\\leftarrow\n",
    "\\theta\n",
    "-\n",
    "\\alpha\n",
    "\\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Approximation de la descente de gradient \n",
    "\n",
    "On peut approximer la formule exacte du gradient en considérant une seule réalisation $(s,a)$.\n",
    "\n",
    "On remplace l’espérance par :\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "\\approx\n",
    "\\left(\n",
    "Q_\\theta(s,a) - Q_\\pi(s,a)\n",
    "\\right)\n",
    "\\nabla_\\theta Q_\\theta(s,a)\n",
    "$$\n",
    "\n",
    "Ce qui conduit à la mise à jour stochastique :\n",
    "\n",
    "$$\n",
    "\\theta\n",
    "\\leftarrow\n",
    "\\theta\n",
    "-\n",
    "\\alpha\n",
    "\\left(\n",
    "Q_\\theta(s,a) - Q_\\pi(s,a)\n",
    "\\right)\n",
    "\\nabla_\\theta Q_\\theta(s,a)\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f462362",
   "metadata": {},
   "source": [
    "## 5. Remplacement de $Q_\\pi$ par une cible TD\n",
    "\n",
    "La quantité $Q_\\pi(s,a)$ est inconnue.\n",
    "\n",
    "On note $(s,a,r,s')$ une transition observée, où $s'$ est l’état obtenu après avoir exécuté l’action $a$ dans l’état $s$.\n",
    "\n",
    "On utilise une approximation bootstrap. On note $y$ cette aproximation de $Q_\\pi(s,a)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Cas SARSA (on-policy)\n",
    "\n",
    "On choisit une action $a'$ selon la politique $\\pi$ au nouvel état $s'$.\n",
    "\n",
    "La cible est :\n",
    "\n",
    "$$\n",
    "y = r + \\gamma Q_\\theta(s', a')\n",
    "$$\n",
    "\n",
    "\n",
    "### Cas Expected SARSA\n",
    "\n",
    "On prend l’espérance sous la politique $\\pi$ :\n",
    "\n",
    "$$\n",
    "y =\n",
    "r + \\gamma  \\sum_{a' \\in  \\mathcal{A}} \\pi(a'\\mid s') Q_{\\theta}(s',a')\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Cas Q-learning (off-policy)\n",
    "\n",
    "On prend la meilleure action au prochain état :\n",
    "\n",
    "$$\n",
    "y =\n",
    "r + \\gamma \\max_{a'} Q_\\theta(s',a')\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d65ca",
   "metadata": {},
   "source": [
    "## 6. Replay Buffer\n",
    "\n",
    "Un replay buffer est une structure de données garder en mémoire afin de mieux estimer l'espérance mentionner précédemment :\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(s,a,r,s')\\}\n",
    "$$\n",
    "\n",
    "Au lieu d’utiliser uniquement la transition courante, on échantillonne un mini-batch aléatoire dans $\\mathcal{D}$.\n",
    "\n",
    "Objectifs :\n",
    "\n",
    "- casser les corrélations temporelles,\n",
    "- stabiliser l’optimisation,\n",
    "- réutiliser les données.\n",
    "\n",
    "On obtient alors une descente de gradient stochastique sur la perte empirique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48528d",
   "metadata": {},
   "source": [
    "## 7. Passage à l’apprentissage batch\n",
    "\n",
    "Dans la mise à jour précédente, l’estimation du gradient repose sur une seule transition.\n",
    "\n",
    "On peut améliorer la stabilité en utilisant un ensemble de transitions\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N\n",
    "$$\n",
    "\n",
    "On définit alors la perte empirique :\n",
    "\n",
    "$$\n",
    "J_N(\\theta)\n",
    "=\n",
    "\\frac{1}{2N}\n",
    "\\sum_{i=1}^N\n",
    "\\left(\n",
    "Q_\\theta(s_i,a_i) - y_i\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "où, pour le cas Q-learning :\n",
    "\n",
    "$$\n",
    "y_i\n",
    "=\n",
    "r_i + \\gamma \\max_{a'} Q_\\theta(s'_i,a')\n",
    "$$\n",
    "\n",
    "Le gradient empirique devient :\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J_N(\\theta)\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N\n",
    "\\left(\n",
    "Q_\\theta(s_i,a_i) - y_i\n",
    "\\right)\n",
    "\\nabla_\\theta Q_\\theta(s_i,a_i)\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2e032",
   "metadata": {},
   "source": [
    "## 8. Approximation linéaire\n",
    "\n",
    "On suppose qu’il existe un vecteur de caractéristiques\n",
    "\n",
    "$$\n",
    "\\phi(s,a) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "et on définit :\n",
    "\n",
    "$$\n",
    "Q_\\theta(s,a)\n",
    "=\n",
    "\\theta^\\top \\phi(s,a)\n",
    "$$\n",
    "\n",
    "avec $\\theta \\in \\mathbb{R}^d$.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "Dans ce cas :\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta Q_\\theta(s,a)\n",
    "=\n",
    "\\phi(s,a)\n",
    "$$\n",
    "\n",
    "Puis on met à jour $\\theta$ par une descente de gradient en approximant les quantités en utilisant les différentes méthodes vues plus haut.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640e024",
   "metadata": {},
   "source": [
    "## 9. Neural Fitted Q-Iteration (NFQ)\n",
    "\n",
    "NFQ est une version batch de Q-learning avec approximation fonctionnelle.\n",
    "\n",
    "Principe :\n",
    "\n",
    "1. Collecter un ensemble de transitions $\\mathcal{D}$.\n",
    "\n",
    "2. Construire les cibles :\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "r_i + \\gamma \\max_{a'} Q_{\\theta_k}(s'_i,a')\n",
    "$$\n",
    "\n",
    "où $\\theta_k$ désigne les paramètres à l’itération précédente.\n",
    "\n",
    "3. Résoudre le problème de régression :\n",
    "\n",
    "$$\n",
    "\\theta_{k+1}\n",
    "=\n",
    "\\arg\\min_\\theta\n",
    "\\frac{1}{N}\n",
    "\\sum_i\n",
    "\\left(\n",
    "Q_\\theta(s_i,a_i) - y_i\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "On alterne ainsi :\n",
    "\n",
    "- calcul des cibles avec les paramètres courants,\n",
    "- résolution d’un problème de régression supervisée,\n",
    "- mise à jour des paramètres.\n",
    "\n",
    "NFQ est donc une procédure itérative de type :\n",
    "\n",
    "$$\n",
    "Q_{k+1}\n",
    "\\approx\n",
    "\\mathcal{T} Q_k\n",
    "$$\n",
    "\n",
    "où $\\mathcal{T}$ est l’opérateur de Bellman optimal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5314f8e",
   "metadata": {},
   "source": [
    "On va commencer par une descente de gradient sur un modèle linéaire dépendant de theta, il approxime la fonction Q. On doit puisqu'on ne conait pas la vraie focntion Q utiliser ce qu'on appelle un bootstrap, les deux différents qui existent sont présentés section 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f32eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3d4b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 4\n",
      "Actions: 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\") \n",
    "# On reprend ici on prend un en vironnment diférents où s est continue et on a que deux actions possibles \n",
    "# (gauche ou droite), le but est de faire tenir le pole en équilibre le plus longtemps possible.\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0]\n",
    "# States ici est le quadruplet (position du chariot, vitesse du chariot, angle du pole, vitesse angulaire du pole)\n",
    "\n",
    "print(\"States:\", n_states)\n",
    "print(\"Actions:\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba015d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On rappelle que l'idée de l'algorithme ici est d'effectuer une descente de gradient pour déterminer \n",
    "# les paramètres theta de la fonction d'action-valeur Q, qui est définie à partir du vecteur de paramètres theta \n",
    "# et de la fonction d'indexation index. Une fois que les paramètres theta sont déterminés, on peut utiliser \n",
    "# la fonction d'action-valeur Q pour déterminer la politique optimale.\n",
    "\n",
    "alpha = 0.1 # taux d'apprentissage pour la descente de gradient\n",
    "gamma = 0.99 # facteur de discount pour les récompenses futures, qui détermine l'importance accordée aux récompenses \n",
    "# futures par rapport aux récompenses immédiates.\n",
    "epsilon = 0.1 # taux d'exploration pour la politique epsilon-greedy, \n",
    "#qui détermine la probabilité d'explorer de nouvelles actions\n",
    "n_episodes = 100000 # nombre d'épisodes d'entraînement, qui détermine le nombre de fois que l'agent \n",
    "# va interagir avec l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2805b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(state, action):\n",
    "    x, x_dot, theta_pole, theta_dot = state\n",
    "    # on utilise l'angle et la position du chariot, ainsi que leurs vitesses respectives, \n",
    "    # pour construire les features de notre fonction d'action-valeur Q.\n",
    "\n",
    "    # On utilise ici une base de fonctions polynomiales pour approximer la fonction d'action-valeur Q.\n",
    "    features = np.array([\n",
    "        1.0,\n",
    "        x,\n",
    "        x_dot,\n",
    "        theta_pole,\n",
    "        theta_dot,\n",
    "        x**2,\n",
    "        x_dot**2,\n",
    "        theta_pole**2,\n",
    "        theta_dot**2,\n",
    "        x * theta_pole,\n",
    "        x_dot * theta_dot\n",
    "    ])\n",
    "\n",
    "    d_state = len(features)\n",
    "\n",
    "    phi_sa = np.zeros(d_state * n_actions) # on crée un vecteur de features pour chaque action possible\n",
    "    # Q_theta(s, a) = theta @ phi(s, a) est simplifiée en theta_a @ features, où theta_a est la partie \n",
    "    # de theta correspondant à l'action a (ici deux actriosn possibles, on a donc deux parties dans theta, \n",
    "    # chacune de taille d_state)\n",
    "\n",
    "    start = action * d_state # début des features pour l'action choisie dans le vecteur phi_sa\n",
    "    phi_sa[start:start+d_state] = features # on remplit les features pour l'action choisie dans \n",
    "    # le vecteur phi_sa, les autres restent à zéro\n",
    "\n",
    "    return phi_sa\n",
    "\n",
    "d = 11 * n_actions\n",
    "theta = np.zeros(d)\n",
    "\n",
    "def Q(theta, state, action):\n",
    "    return theta @ phi(state, action) # la fonction d'action-valeur Q est définie comme le produit \n",
    "# scalaire entre les paramètres theta et les features phi(state, action), le but est toujours de trouver \n",
    "# les paramètres theta qui permettent d'approximer au mieux la fonction d'action-valeur Q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6075236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matys Savidan\\AppData\\Local\\Temp\\ipykernel_4896\\1017138933.py:38: RuntimeWarning: overflow encountered in matmul\n",
      "  return theta @ phi(state, action) # la fonction d'action-valeur Q est définie comme le produit\n",
      "C:\\Users\\Matys Savidan\\AppData\\Local\\Temp\\ipykernel_4896\\845353009.py:55: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  td_error = y - Q(theta, s, a) # calcul de l'erreur temporelle-différence (TD error),\n"
     ]
    }
   ],
   "source": [
    "for episode in range(n_episodes):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < epsilon: # avec une probabilité epsilon, on choisit une action \n",
    "            # aléatoire pour encourager l'exploration\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = [Q(theta, s, a_) for a_ in range(n_actions)]\n",
    "            a = np.argmax(q_values) # on choisit l'action qui maximise la fonction \n",
    "            # d'action-valeur Q pour l'état actuel s\n",
    "\n",
    "        s_next, r, terminated, truncated, _ = env.step(a) # on exécute l'action choisie et on observe la transition\n",
    "        # vers le nouvel état s_next, la récompense r, et les indicateurs de fin d'épisode terminated et truncated\n",
    "        done = terminated or truncated # on vérifie si l'épisode est terminé\n",
    "\n",
    "        \n",
    "        # Target Q-learning\n",
    "        q_next = max(Q(theta, s_next, a_) for a_ in range(n_actions))\n",
    "        y = r + gamma * q_next #réalisation du bootstrap  (concept clef ici) pour calculer la cible y, \n",
    "        # qui est la récompense immédiate r plus la valeur maximale de Q pour le nouvel état s_next, estimée \n",
    "        # à partir de la fonction d'action-valeur Q actuelle et du facteur de discount gamma. On utilise \n",
    "        # l'approximation Q par le max (Q learning) pour estimer la valeur future et non la somme pondérée (SARSA).\n",
    "        # On devrait modifier la mise à jour de q_next = sum pi(a|s_next) * Q(theta, s_next, a) for a in range(n_actions) \n",
    "        # pour faire du SARSA, mais ici on fait du Q-learning, qui est plus simple et plus rapide à calculer.\n",
    "        \n",
    "        \"\"\"\n",
    "        q_values = [Q(theta, s_next, a_) for a_ in range(n_actions)]\n",
    "        q_max = max(q_values)\n",
    "        q_min = min(q_values)\n",
    "        expected_q = (1 - epsilon + epsilon/2) * q_max + (epsilon/2) * q_min\n",
    "        y = r + gamma * expected_q\n",
    "        # il n'y a que deux action possible, on peut faire une moyenne pondérée entre la valeur maximale et \n",
    "        # la valeur minimale de Q pour le nouvel état s_next, en utilisant les probabilités d'exploration et d'exploitation \n",
    "        # de la politique epsilon-greedy, \n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            a_next = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = [Q(theta, s_next, a_) for a_ in range(n_actions)]\n",
    "            a_next = np.argmax(q_values)\n",
    "\n",
    "        q_next = Q(theta, s_next, a_next) # pour faire du SARSA, on utilise la même action a pour calculer la valeur future,\n",
    "        y = r + gamma * q_next # la cible y est calculée en utilisant la récompense immédiate r plus la valeur de Q pour \n",
    "        # le nouvel état s_next et l'action a, ce qui rend l'algorithme plus stable mais aussi plus lent à converger, \n",
    "        # #car on utilise une estimation plus précise de la valeur future, mais qui peut être plus bruit\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        td_error = y - Q(theta, s, a) # calcul de l'erreur temporelle-différence (TD error), \n",
    "        # qui mesure la différence entre la cible y et l'estimation actuelle de Q pour l'état s et l'action a,\n",
    "        # cette erreur est utilisée pour mettre à jour les paramètres theta de la fonction d'action-valeur Q.\n",
    "        # C'est une approximation de l'espérance par une seule réalisation, ce qui rend l'algorithme plus efficace\n",
    "        # en termes de calcul. \n",
    "\n",
    "        # Mise à jour linéaire \n",
    "        theta += alpha * td_error * phi(s, a) # mise à jour des paramètres theta en utilisant l'erreur TD, \n",
    "        # le taux d'apprentissage alpha, et les features phi(s, a) (gradient de Q ici) pour l'état s et l'action a, \n",
    "        # ce qui permet d'ajuster les paramètres theta pour mieux approximer la fonction d'action-valeur Q.\n",
    "        # On utlise l'approximation de l'espérance par une seule réalisation \n",
    "\n",
    "        s = s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97d5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(theta, state):\n",
    "    q_values = [Q(theta, state, a) for a in range(n_actions)]\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "# Ici on extrait la politique finale à partir des paramètres theta appris, en choisissant pour chaque \n",
    "# état l'action qui maximise la fonction d'action-valeur Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce693233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, theta, n_eval=50):\n",
    "    rewards = []\n",
    "    #Ici regarde le reward moyen obtenu par la politique apprise sur un certain nombre \n",
    "    # d'épisodes d'évaluation n_eval. Dans cette modélisation les récompenses sont de 1 à chaque étape \n",
    "    # tant que le pole ne tombe pas. \n",
    "\n",
    "    for _ in range(n_eval):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done: # tant que le pole n'est pas tombé\n",
    "            action = policy(theta, state) # on choisit l'action selon la politique apprise \n",
    "            state, reward, terminated, truncated, _ = env.step(action) # applique l'action à l'environnement \n",
    "            # et observe la transition\n",
    "            done = terminated or truncated # vérifie si l'épisode est terminé\n",
    "            total_reward += reward # accumule la récompense totale obtenue dans cet épisode\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return np.mean(rewards) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610f315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 9.18\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward:\", evaluate(env, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae85f8d",
   "metadata": {},
   "source": [
    "On a donc implémenté une version d'approximation linéaire de Q. On a vu que le reward moyen n'était pas exeptionnel, nous allosn docn implémenté une version avec Replay Buffer pour améliorer l'estimation de l'espérance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5555bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "def add_to_buffer(s, a, r, s_next, done):\n",
    "    replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "def sample_batch():\n",
    "    return random.sample(replay_buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f5c9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matys Savidan\\AppData\\Local\\Temp\\ipykernel_12464\\1017138933.py:38: RuntimeWarning: overflow encountered in matmul\n",
      "  return theta @ phi(state, action) # la fonction d'action-valeur Q est définie comme le produit\n",
      "C:\\Users\\Matys Savidan\\AppData\\Local\\Temp\\ipykernel_12464\\2413065300.py:41: RuntimeWarning: invalid value encountered in multiply\n",
      "  theta += alpha * td_error * phi(s_b, a_b)\n",
      "C:\\Users\\Matys Savidan\\AppData\\Local\\Temp\\ipykernel_12464\\1017138933.py:38: RuntimeWarning: invalid value encountered in matmul\n",
      "  return theta @ phi(state, action) # la fonction d'action-valeur Q est définie comme le produit\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(d) #on réinitialise les paramètres theta à zéro pour entraîner un \n",
    "# nouveau modèle à partir du dataset collecté\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = [Q(theta, s, a_) for a_ in range(n_actions)]\n",
    "            a = np.argmax(q_values)\n",
    "\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Ajouter au buffer\n",
    "        add_to_buffer(s, a, r, s_next, done)\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "        # Apprentissage si assez de données\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "\n",
    "            batch = sample_batch() #baych aléatoire de transitions (s, a, r, s_next, done) est prélevé du \n",
    "            # replay buffer pour entraîner le modèle en evaluant l'espérance par une moyenne sur un batch de\n",
    "            #  transitions plutôt que par une seule réalisation.\n",
    "\n",
    "            theta_old = theta.copy()          # θ figé pour calcul des cibles\n",
    "            grad = np.zeros_like(theta)\n",
    "\n",
    "            for (s_b, a_b, r_b, s_next_b, done_b) in batch: # on somme sur les transitions du batch pour \n",
    "                #calculer la mise à jour des paramètres theta, ce qui permet d'obtenir une estimation plus \n",
    "                # stable de la cible y et de l'erreur TD, en réduisant la variance par rapport à l'utilisation \n",
    "                # d'une seule transition.\n",
    "\n",
    "                if done_b:\n",
    "                    y = r_b\n",
    "                else:\n",
    "                    q_next = max(Q(theta_old, s_next_b, a_)\n",
    "                                 for a_ in range(n_actions)) \n",
    "                    y = r_b + gamma * q_next # on applique la même formule de bootstrap que précédement pour \n",
    "                    # calculer la cible y, mais cette fois-ci en utilisant les transitions du batch prélevé du \n",
    "                    # replay buffer. \n",
    "\n",
    "                td_error = y - Q(theta_old, s_b, a_b) # calcul de l'erreur temporelle-différence (TD error) \n",
    "                # pour la transition du batch, qui mesure la différence entre la cible y et l'estimation\n",
    "                # actuelle de Q. \n",
    "\n",
    "                grad += td_error * phi(s_b, a_b)\n",
    "\n",
    "            theta += (alpha/batch_size) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a25bb481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward (replay): 9.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward (replay):\", evaluate(env, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409ef13",
   "metadata": {},
   "source": [
    "## Passage au NFQ\n",
    "\n",
    "On observe une non-amélioration du reward moyen avec le replay buffer.  \n",
    "Cela indique que le problème ne venait pas de l’estimation de l’espérance ni de la variance du gradient, mais plutôt de la capacité du modèle ou de la structure de l’optimisation.\n",
    "\n",
    "Nous passons donc au NFQ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b4f09",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "Avec un replay buffer :\n",
    "\n",
    "- On échantillonne un mini-batch.\n",
    "- On calcule les cibles avec les paramètres courants.\n",
    "- On calcule le gradient et on fait un pas de descente.\n",
    "- On met à jour les paramètres.\n",
    "- On recommence immédiatement.\n",
    "\n",
    "Les cibles sont donc recalculées fréquemment.\n",
    "\n",
    "L’optimisation est incrémentale.\n",
    "\n",
    "Il n’y a pas de séparation stricte entre la construction des cibles et l’optimisation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e824c8",
   "metadata": {},
   "source": [
    "## NFQ\n",
    "\n",
    "Dans le NFQ, on travaille sur l’ensemble du dataset d’un seul coup.\n",
    "\n",
    "À l’itération k :\n",
    "\n",
    "1. On fixe les paramètres $\\theta_k$\n",
    "\n",
    "2. On construit toutes les cibles :\n",
    "\n",
    "$$\n",
    "y_i^{(k)} = r_i + \\gamma \\max_{a'} Q_{\\theta_k}(s'_i, a')\n",
    "$$\n",
    "\n",
    "3. On optimise complètement le problème (soit par descente de gradient itérative durant laquelle les cibles ne changent  pas soit par résolution d'un système linéaire) :\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_i \\left( Q_\\theta(s_i, a_i) - y_i^{(k)} \\right)^2\n",
    "$$\n",
    "\n",
    "Pendant cette optimisation :\n",
    "\n",
    "- les cibles restent fixes,\n",
    "- seule la fonction d’approximation évolue.\n",
    "\n",
    "On obtient ensuite $\\theta_{k+1}$\n",
    "\n",
    "Puis on reconstruit de nouvelles cibles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb36d76",
   "metadata": {},
   "source": [
    "## Interprétation structurelle\n",
    "\n",
    "Replay buffer  \n",
    "→ optimisation incrémentale avec recalcul fréquent des cibles.\n",
    "\n",
    "NFQ  \n",
    "→ application de l’opérateur de Bellman puis projection complète.\n",
    "\n",
    "La dynamique correspond à :\n",
    "\n",
    "$$\n",
    "Q_{k+1} = \\Pi(\\mathcal{T} Q_k)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dataset(env, n_transitions=50000):\n",
    "    dataset = []\n",
    "    state, _ = env.reset() # on initialise l'environnement et on obtient le premier état,\n",
    "    # qui est un vecteur de taille 4\n",
    "\n",
    "    while len(dataset) < n_transitions:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        dataset.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    return dataset \n",
    "\n",
    "# l'idée ici est de collecter un dataset de transitions (s, a, r, s_next, done) en interagissant avec l'environnement\n",
    "\n",
    "dataset = collect_dataset(env, n_transitions=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7be1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(d) # on réinitialise les paramètres theta à zéro pour entraîner un \n",
    "# nouveau modèle à partir du dataset collecté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bcc2a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfq_iteration(theta, dataset, gamma, ridge=1e-4):\n",
    "    # le ridge c'ets juste pour la régularisation de la solution de régression linéaire, \n",
    "    # qui permet d'éviter la non inversibilité de la matrice dans la solution de régression linéaire\n",
    "\n",
    "    N = len(dataset)\n",
    "    Phi = np.zeros((N, len(theta)))\n",
    "    y = np.zeros(N)\n",
    "\n",
    "    # Construction des cibles\n",
    "    for i, (s, a, r, s_next, done) in enumerate(dataset):\n",
    "\n",
    "        Phi[i] = phi(s, a)\n",
    "\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            q_next = max(Q(theta, s_next, a_) for a_ in range(n_actions))\n",
    "            target = r + gamma * q_next # construction des cibles y à partir des transitions du dataset, \n",
    "            # en utilisant la même formule de bootstrap que précédement.\n",
    "\n",
    "        y[i] = target\n",
    "\n",
    "    # Solution ridge : (ΦᵀΦ + λI)θ = Φᵀy\n",
    "    A = Phi.T @ Phi + ridge * np.eye(len(theta))\n",
    "    b = Phi.T @ y\n",
    "    # résolution du système linéaire pour trouver les paramètres theta qui minimisent l'erreur \n",
    "    # quadratique entre les cibles y et les prédictions Phi @ theta, ce qui correspond à une \n",
    "    # étape de régression linéaire pour ajuster les paramètres theta de la fonction d'action-valeur Q \n",
    "    # en fonction du dataset collecté.\n",
    "\n",
    "    theta_new = np.linalg.solve(A, b)\n",
    "    \n",
    "\n",
    "    return theta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1d9fa747",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "\n",
    "for k in range(n_iterations):\n",
    "    theta = nfq_iteration(theta, dataset, gamma)\n",
    "    # On itère plusieurs fois sur le dataset pour affiner les paramètres theta, \n",
    "    # ce qui permet d'améliorer l'approximation de la fonction d'action-valeur Q à partir du dataset collecté.\n",
    "    # entre chaque itération, les paramètres theta sont mis à jour en fonction du dataset collecté, \n",
    "    # ce qui permet d'améliorer l'approximation de la fonction d'action-valeur Q à partir du dataset collecté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37cf5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 173.35\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward:\", evaluate(env, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50368c08",
   "metadata": {},
   "source": [
    "On remarque un reward bien superieur et un temps de calcul comparable à la version sans replay buffer. C'est une $\\textcolor{red}{\\text{large}}$ victoire. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
